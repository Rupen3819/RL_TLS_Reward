{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###pip install -r requirements.txt \n",
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import traci\n",
    "import timeit\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from utils import import_train_configuration\n",
    "from shutil import copyfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Check the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "{'gui': False, 'total_episodes': 5, 'max_steps': 3600, 'n_cars_generated': 1000, 'green_duration': 10, 'yellow_duration': 3, 'num_layers': None, 'width_layers': None, 'batch_size': 256, 'learning_rate': 0.0001, 'training_epochs': 100, 'target_update': 3, 'memory_size_min': 600, 'memory_size_max': 100000, 'eps_start': 1.0, 'eps_end': 0.05, 'eps_decay': 0.999, 'num_states': 17, 'num_actions': 8, 'gamma': 0.999, 'hidden_dim': ['64', '64'], 'tau': 0.001, 'models_path_name': 'models', 'sumocfg_file_name': 'dummy\\\\sumo_config.sumocfg', 'generation_process': 'random', 'state_representation': 'amount_cars', 'action_representation': 'choose_light', 'agent_type': 'DQN', 'model': 'model', 'reward_definition': 'waiting_cars'}\n"
     ]
    }
   ],
   "source": [
    "config = import_train_configuration(config_file='training_settings.ini')\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'red_duration'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-4-0f7865390763>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mEnvironment\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSUMO_train\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSUMO\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0menv\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mSUMO\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'State shape: '\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mobservation_space\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Number of actions: '\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0menv\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction_space\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Documents\\RL_TLS_Main_Intersections\\Environment\\SUMO_train.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     30\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgreen_duration\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'green_duration'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0myellow_duration\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'yellow_duration'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 32\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mred_duration\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'red_duration'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     33\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjunction_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0minit_states\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTL_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'red_duration'"
     ]
    }
   ],
   "source": [
    "from Environment.SUMO_train import SUMO\n",
    "env=SUMO()\n",
    "print('State shape: ', env.observation_space.shape[0])\n",
    "print('Number of actions: ', env.action_space.n)\n",
    "\n",
    "from dqn_agent import Agent\n",
    "\n",
    "agent = Agent(config['num_states'], config['num_actions'], config['hidden_dim'],\n",
    "              config['memory_size_max'], config['batch_size'], config['gamma'], config['tau'],\n",
    "              config['learning_rate'],config['target_update'],\n",
    "              seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = set_train_path(config['models_path_name'])\n",
    "print('Training results will be saved in:',path)\n",
    "\n",
    "\n",
    "#Define the RL training loop\n",
    "def RL(n_episodes=config['total_episodes'], max_t=config['max_steps']+1000, eps_start=config['eps_start'], eps_end=config['eps_end'], eps_decay=config['eps_decay']):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "\n",
    "    timestamp_start = datetime.datetime.now()\n",
    "    training_time=[]\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        from generator import TrafficGenerator\n",
    "        TrafficGen = TrafficGenerator(\n",
    "                3600, \n",
    "                1000\n",
    "            )\n",
    "\n",
    "        TrafficGen.generate_routefile(seed=i_episode)\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            q_values,action = agent.act(np.array(state), eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        training_time.append((datetime.datetime.now()-timestamp_start).total_seconds()) \n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\r                                       Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=500:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    torch.save(agent.qnetwork_local.state_dict(), os.path.join(path, 'checkpoint.pth'))\n",
    "    env.close()\n",
    "    return scores, training_time\n",
    "\n",
    "\n",
    "#Run the training\n",
    "scores, training_time = RL()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.savefig(os.path.join(path, 'training_reward.png'))\n",
    "plt.show()\n",
    "copyfile(src='training_settings.ini', dst=os.path.join(path, 'training_settings.ini'))\n",
    "DataFrame(data={\"reward\":scores}).to_csv(os.path.join(path, 'reward.csv'),sep=',')\n",
    "DataFrame(data={\"reward\":scores,\"training_time\":training_time,\n",
    "               \"waiting_time\":env._cumulative_wait_store,\"avg_queue_length\":env._avg_queue_length_store}\n",
    "         ).to_csv(os.path.join(path, 'training_stats.csv'),sep=',',index=False)\n",
    "add_masterdata(path, config, scores, training_time, env._cumulative_wait_store, env._avg_queue_length_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "path=Path(\"models/model_14\")\n",
    "agent.qnetwork_local.load_state_dict(torch.load(os.path.join(path, 'checkpoint.pth')))\n",
    "\n",
    "# Load the test environment\n",
    "from Environment.SUMO_test import SUMO_test\n",
    "#traci.close()\n",
    "#env=SUMO_test()\n",
    "scenarios=['5-6_(1380)', '8-9_(2600)', '17-18_(3100)', '23-24_(470)']\n",
    "test_cases=[Path('dummy/sumo_test1.sumocfg'), Path('dummy/sumo_test2.sumocfg'),Path('dummy/sumo_test3.sumocfg'),Path('dummy/sumo_test4.sumocfg')]\n",
    "if not os.path.exists(os.path.join(path, 'test')):\n",
    "    os.mkdir(os.path.join(path, 'test'))\n",
    "for t, scenario in enumerate(scenarios):\n",
    "    env.close()\n",
    "    env=SUMO_test(test_cases[t])\n",
    "    #env._sumo_cmd= set_sumo(config['gui'], test_cases[t], config['max_steps'])\n",
    "    #print(env._sumo_cmd)\n",
    "    #print(traci.vehicle.getIDList())\n",
    "    state = env.reset()\n",
    "    path_test=os.path.join(path,'test',scenario)\n",
    "    print(path_test)\n",
    "    _qvalues=list()\n",
    "    for j in range(config['max_steps']+1000):\n",
    "        q_values, action = agent.act(np.array(state))\n",
    "        q_values.append(q_values.append(traci.simulation.getTime()))\n",
    "        env.render()\n",
    "        #print(traci.simulation.getTime())\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break \n",
    "            traci.close()\n",
    "    if not os.path.exists(path_test):   \n",
    "        os.mkdir(path_test)        \n",
    "    df_position=pd.DataFrame(env._positions, columns=['x-position', 'y-position', 'step'])\n",
    "    df_position.to_csv(os.path.join(path_test, 'position.csv'), index=False)\n",
    "\n",
    "    df_emission=pd.DataFrame(env._emissions, columns=['emission', 'step'])\n",
    "    df_emission.to_csv(os.path.join(path_test, 'emission.csv'), index=False)\n",
    "\n",
    "    df_emission=pd.DataFrame(_qvalues,  columns=['action 1', 'action 2', 'action 3', 'action 4', 'action 5', 'action 6', 'action 7', 'action 8', 'time'])\n",
    "    df_emission.to_csv(os.path.join(path_test, 'q_list.csv'), index=False)\n",
    "\n",
    "    df_emission=pd.DataFrame(env._waiting, columns=['waiting time','step'])\n",
    "    df_emission.to_csv(os.path.join(path_test, 'waiting.csv'), index=False)\n",
    "\n",
    "    df_emission=pd.DataFrame(env._waiting_cars, columns=['car id','waiting time car'])\n",
    "    df_emission.to_csv(os.path.join(path_test, 'waiting_car.csv'), index=False)\n",
    "        \n",
    "    df_emission=pd.DataFrame(env._actions, columns=['action', 'step'])\n",
    "    df_emission.to_csv(os.path.join(path_test, 'action.csv'), index=False)\n",
    "\n",
    "    df_emission=pd.DataFrame(env._rewards, columns=['reward', 'step'])\n",
    "    df_emission.to_csv(os.path.join(path_test, 'reward_replay.csv'), index=False)\n",
    "    \n",
    "    df_emission=pd.DataFrame(env._state_memory, columns=['N2TL_0','N2TL_1','N2TL_2','N2TL_3',\n",
    "                 'E2TL_0', 'E2TL_1','E2TL_2','E2TL_3',\n",
    "                 'S2TL_0', 'S2TL_1', 'S2TL_2', 'S2TL_3',\n",
    "                 'W2TL_0', 'W2TL_1', 'W2TL_2', 'W2TL_3','TL'])\n",
    "    df_emission.to_csv(os.path.join(path_test, 'state_memory.csv'), index=False)\n",
    "    \n",
    "    \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3, 5, 6, 7]\n",
    "numbers.insert(0,0)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"hello\"\n",
    "globals()[name] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists=[[]]*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-fe32ca3b",
   "language": "python",
   "display_name": "PyCharm (RL_TLS_Main_Intersections)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}