# TLS_RL
This Repository is a modular implementation for Deep Reinforcement Learning (DRL) to optimize taffic light control. The focus is on the optimization of single traffic nodes or a smaller group of traffic nodes. The optimization should therefore be performed by a single agent.

The research goals of this project are in the first step the evaluation of different approaches (state-space, action-space, rl-algorithm etc.) of past research with real traffic data from a traffic simulation of Ingolstadt consisting of traffic volume, topology and current switching operations. Special focus is put on the optimization of a multimodal traffic flow. For this purpose, the Multimodal Perforcmance Index (MPI) developed by the TUM will be used and evaluated for the application in RL context. Further research topics are fairness, interpretability and the integration of vehicle functions (e.g. Auto Info Online (AIO)) in the context of the optimization of Amel circuits by DRL. 

## Installation
To run the code in this repository, one needs to install [Python 3.8 or](https://www.python.org/downloads/) higher and [Anaconda](https://docs.anaconda.com/anaconda/install/index.html). If both is instastalled, clone (download) this repository and install the required packages with
```ruby
pip -r requirements.txt
```
## Strukture of this Repository
![image](https://user-images.githubusercontent.com/84088115/129562261-71116b9d-01fd-4613-9e01-5b691d1506fe.png)

This repository follows a Train-Test-Evaluate strukture. First, the desired RL Agent with the (hyper)parameters will be trained in a specific traffic setup. Traffic setup hereby refers to both the topology of the road network and the traffic load. After training, the weights of the neural network(s) will be saved autamatically to test the trained agent in specific traffic load situations. Various data are recorded, such as the waiting times at the various points in time of the simulation. This data can then be compared and evaluated against various other agents in the evaluate agent step using an interactive dashboard. 
### Train Agent
The first step to train a reinforcement learning agent to control the traffic lights is to define the relevant (hyper)parameters and settings. These are defined in the 
```ruby
training_settings.ini
```
file. In the current implementation, the traffic load for training is defined with the number by the number of timesteps done in simulation as well as the number of cars generated during that time. The cars are then generated by a Weibull distribution. The car generation process is done in ``` generator.py ```.Once the settings are defined, the agent can be trained using the 
```ruby
train.py
```
Python script. After the agent trained for the defined amout of episodes, a new subfolder will be created under /models containing the saved parameters of the neural network(s), a plot of the reward during taining, a copy of the training_settings.ini file with the current configuration and a file with the training stats.
### Test Agent
```ruby
Testing the Agent is currently WIP
```
After training the agent, it can be tested with a test dataset, which is a specific traffic load taken from a simulation of Ingolstadt. In order to represent different loads, a traffic volume with low (23-24 o'clock), two medium (5-6 o'clock and 8-9 o'clock) as well as a peak (17-18 o'clock) traffic volume were used here. To record the performance of the trained agent with different traffic loads run the last two cells of
```ruby
Deep_Q_Network_Edit.ipynb.
```
The resulting data will automatically be saved in the /models folder.
### Evaluate Agent
For evaluating the agent, we developed a Dashboard. The Dashbaord gives the possibility to compare different trained agents (i.e. trained with different Hyperparameters) using the data collected from the test agent step. To comapre different agents, first copy the agents data from the /models folder to the /Data folder. Afterwards, open a Terminal window, navigate to the project folder and run the Start_Dashboard.py file (python Start_Dashboard.py). 
## SUMO Gym Environment
